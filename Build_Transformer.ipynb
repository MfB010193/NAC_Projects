{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fef9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the nuccesary libraries and packages are imported\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import LayerNorm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "print('all the nuccesary libraries and packages are imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5385144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: tiny_shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "DATA_FILE = \"tiny_shakespeare.txt\"\n",
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "print(\"Dataset already exists:\", DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf34249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class HParams:\n",
    "    block_size: int = 64\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    d_model: int = 128\n",
    "    d_ff: int = 512\n",
    "    dropout: float = 0.1\n",
    "    emb_dropout: float = 0.0\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    batch_size: int = 16\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 500\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    grad_clip: float = 1.0\n",
    "    top_k: int = 40\n",
    "    temperature: float = 1.0\n",
    "\n",
    "hps = HParams()\n",
    "print(\"Using device:\", hps.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90f64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_vocab(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "    itos = {i:ch for ch,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + 1 + self.block_size]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd60f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "set_seed(hps.seed)\n",
    "\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "stoi, itos = build_vocab(text)\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "train_ds = CharDataset(train_data, hps.block_size)\n",
    "val_ds = CharDataset(val_data, hps.block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=hps.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=hps.batch_size)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88587001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.proj(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hps):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(hps.d_model)\n",
    "        self.attn = CausalSelfAttention(hps.d_model, hps.n_heads, hps.block_size, hps.dropout)\n",
    "        self.ln2 = LayerNorm(hps.d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hps.d_model, hps.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hps.d_ff, hps.d_model),\n",
    "            nn.Dropout(hps.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, hps):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, hps.d_model)\n",
    "        self.pos_emb = nn.Embedding(hps.block_size, hps.d_model)\n",
    "        self.blocks = nn.Sequential(*[Block(hps) for _ in range(hps.n_layers)])\n",
    "        self.ln = LayerNorm(hps.d_model)\n",
    "        self.head = nn.Linear(hps.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size()\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        x = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f70b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 421697\n",
      "Epoch 1 | Step 0 | Loss 4.3895\n",
      "Epoch 1 | Step 100 | Loss 2.6942\n",
      "Epoch 1 | Step 200 | Loss 2.6674\n",
      "Epoch 1 | Step 300 | Loss 2.5395\n",
      "Epoch 1 | Step 400 | Loss 2.5116\n",
      "Epoch 1 | Step 500 | Loss 2.4569\n",
      "Epoch 1 | Step 600 | Loss 2.4013\n",
      "Epoch 1 | Step 700 | Loss 2.4101\n",
      "Epoch 1 | Step 800 | Loss 2.2895\n",
      "Epoch 1 | Step 900 | Loss 2.2200\n",
      "Epoch 1 | Step 1000 | Loss 2.2228\n",
      "Epoch 1 | Step 1100 | Loss 2.2287\n",
      "Epoch 1 | Step 1200 | Loss 2.2798\n",
      "Epoch 1 | Step 1300 | Loss 2.1802\n",
      "Epoch 1 | Step 1400 | Loss 2.1679\n",
      "Epoch 1 | Step 1500 | Loss 2.1066\n",
      "Epoch 1 | Step 1600 | Loss 2.1370\n",
      "Epoch 1 | Step 1700 | Loss 2.0892\n",
      "Epoch 1 | Step 1800 | Loss 2.0576\n",
      "Epoch 1 | Step 1900 | Loss 2.0118\n",
      "Training finished ✅\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(hps.device)\n",
    "model = TinyGPT(vocab_size, hps).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hps.lr)\n",
    "\n",
    "print(\"Total parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), hps.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "        step += 1\n",
    "        if step >= hps.max_iters:\n",
    "            break\n",
    "    if step >= hps.max_iters:\n",
    "        break\n",
    "\n",
    "print(\"Training finished ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380b4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
